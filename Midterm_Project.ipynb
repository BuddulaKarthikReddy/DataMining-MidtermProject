{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Av80a7RDFRdQ"
   },
   "source": [
    "## Installing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hh1Iw6_5FNDd"
   },
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEsKu3gCDUck"
   },
   "source": [
    "# Generating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Z3gIHCuiDRFO",
    "outputId": "f6b69567-3681-4a83-f8e0-68b9c61a2fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been created for Grocery Store: Grocery Store_items.csv and Grocery Store_transactions.csv\n",
      "CSV files have been created for Electronics Store: Electronics Store_items.csv and Electronics Store_transactions.csv\n",
      "CSV files have been created for Clothing Store: Clothing Store_items.csv and Clothing Store_transactions.csv\n",
      "CSV files have been created for Sports Equipment Store: Sports Equipment Store_items.csv and Sports Equipment Store_transactions.csv\n",
      "CSV files have been created for Bookstore: Bookstore_items.csv and Bookstore_transactions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set a specific random seed to create deterministic transactions\n",
    "random.seed(0)\n",
    "\n",
    "# Function to generate and save datasets\n",
    "def generate_store_dataset(store_name, items):\n",
    "    # Create an empty list to store transactions\n",
    "    transactions = []\n",
    "\n",
    "    # Generate 20 transactions with random items\n",
    "    for i in range(1, 21):\n",
    "        num_items = random.randint(1, 10)  # Number of items in each transaction\n",
    "        transaction = random.sample(items, num_items)\n",
    "        transaction_str = ', '.join(transaction)\n",
    "        transactions.append([f\"Trans{i}\", transaction_str])\n",
    "\n",
    "    # Create DataFrames\n",
    "    df_items = pd.DataFrame(items, columns=['Item Name'])\n",
    "    df_transactions = pd.DataFrame(transactions, columns=['Transaction ID', 'Transaction Items'])\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    df_items.to_csv(f\"{store_name}_items.csv\", index=False)\n",
    "    df_transactions.to_csv(f\"{store_name}_transactions.csv\", index=False)\n",
    "\n",
    "    print(f\"CSV files have been created for {store_name}: {store_name}_items.csv and {store_name}_transactions.csv\")\n",
    "\n",
    "# List of stores and their items\n",
    "stores = {\n",
    "    \"Grocery Store\": ['Milk', 'Bread', 'Eggs', 'Cheese', 'Apples', 'Bananas', 'Chicken', 'Rice', 'Pasta', 'Tomatoes'],\n",
    "    \"Electronics Store\": ['Laptop', 'Smartphone', 'Headphones', 'Smartwatch', 'Tablet', 'Portable Charger', 'Camera', 'Laptop Bag', 'Monitor', 'Keyboard'],\n",
    "    \"Clothing Store\": ['Jeans', 'T-Shirt', 'Dress', 'Jacket', 'Scarf', 'Hat', 'Sneakers', 'Socks', 'Belt', 'Sweater'],\n",
    "    \"Sports Equipment Store\": ['Football', 'Basketball', 'Tennis Racket', 'Baseball Glove', 'Golf Balls', 'Yoga Mat', 'Running Shoes', 'Swim Goggles', 'Fitness Tracker', 'Water Bottle'],\n",
    "    \"Bookstore\": ['Fiction Novel', 'Science Textbook', 'History Book', 'Biography', 'Poetry Collection', 'Mystery Novel', 'Science Fiction Novel', 'Cookbook', 'Art Book', 'Children\\'s Book']\n",
    "}\n",
    "\n",
    "# Generate datasets for each store\n",
    "for store, items in stores.items():\n",
    "    generate_store_dataset(store, items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7Yox5KyCmyj"
   },
   "source": [
    "# Association Rule Mining Analysis\n",
    "\n",
    "This script performs association rule mining using three different methods: a custom Brute Force approach, the Apriori algorithm, and the FP-Growth algorithm, comparing their performance on a selected dataset. First, we load the transaction data from a specified store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "T75IQHY2CoMq",
    "outputId": "b21bdbd6-1a5f-494b-e303-acc56c4a976c"
   },
   "outputs": [],
   "source": [
    "# import libraries and load data\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
    "import time\n",
    "\n",
    "def load_transactions(filename):\n",
    "    \"\"\" Load and preprocess transactions from a CSV file. \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    transactions = df['Transaction Items'].apply(lambda x: x.split(','))\n",
    "    return transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsN02BknCztn"
   },
   "source": [
    "## Brute Force Method\n",
    "\n",
    "Implementing the brute force method to generate frequent itemsets. This method evaluates every possible combination of items to determine which sets meet the minimum support threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "iMGmb6dHC1kG",
    "outputId": "dcc61da1-4745-4f97-c6a6-076389d56213"
   },
   "outputs": [],
   "source": [
    "# Brute Force Functino\n",
    "\n",
    "def brute_force_frequent_itemsets(transactions, min_support):\n",
    "    \"\"\" Efficiently generate frequent itemsets using the brute force method. \"\"\"\n",
    "    item_set = set(itertools.chain.from_iterable(transactions))\n",
    "    item_list = list(item_set)\n",
    "    frequent_itemsets = []\n",
    "    num_transactions = len(transactions)\n",
    "    transaction_sets = [set(transaction) for transaction in transactions]\n",
    "\n",
    "    for r in range(1, len(item_list) + 1):\n",
    "        for itemset in itertools.combinations(item_list, r):\n",
    "            itemset_support = sum(1 for transaction in transaction_sets if set(itemset).issubset(transaction)) / num_transactions\n",
    "            if itemset_support >= min_support:\n",
    "                frequent_itemsets.append((itemset, itemset_support))\n",
    "\n",
    "    return pd.DataFrame(frequent_itemsets, columns=[\"itemsets\", \"support\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fAHkJbDC-j1"
   },
   "source": [
    "## Algorithm Execution\n",
    "\n",
    "Here, we execute the Apriori and FP-Growth algorithms using the `mlxtend` library, and measure the execution time for each. We also include the Brute Force method executed previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "uU3v1j4MC9aJ",
    "outputId": "d822a87a-5903-4b71-afb7-88c5d2d0754e"
   },
   "outputs": [],
   "source": [
    "def run_mlxtend_algorithm(transactions, min_support, algorithm):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    if algorithm == 'apriori':\n",
    "        return apriori(df, min_support=min_support, use_colnames=True)\n",
    "    else:\n",
    "        return fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "def generate_rules(frequent_itemsets, min_confidence):\n",
    "    if not frequent_itemsets.empty:\n",
    "        return association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def generate_association_rules(frequent_itemsets, transactions, min_support, min_confidence):\n",
    "    \"\"\"Generate and print association rules from frequent itemsets.\"\"\"\n",
    "    rules = []\n",
    "    transaction_list = list(map(set, transactions))\n",
    "    total_transactions = len(transaction_list)\n",
    "\n",
    "    for itemsets, support in frequent_itemsets.itertuples(index=False):\n",
    "        # Get all non-empty subsets of the itemset\n",
    "        for consequent in itertools.chain.from_iterable(itertools.combinations(itemsets, r) for r in range(1, len(itemsets))):\n",
    "            antecedent = itemsets.difference(consequent)\n",
    "            antecedent_support = get_support(antecedent, transaction_list) / total_transactions\n",
    "            consequent_support = get_support(consequent, transaction_list) / total_transactions\n",
    "\n",
    "            if antecedent_support > 0:  # Avoid division by zero\n",
    "                confidence = support / antecedent_support\n",
    "                if confidence >= min_confidence and support >= min_support:\n",
    "                    rules.append((antecedent, consequent, support, confidence, consequent_support))\n",
    "\n",
    "    # Print rules\n",
    "    print(\"Generated Association Rules:\")\n",
    "    for rule in rules:\n",
    "        print(f\"Rule: {rule[0]} -> {rule[1]}, Support: {rule[2]}, Confidence: {rule[3]}, Lift: {rule[4]/rule[2]}\")\n",
    "\n",
    "def get_support(itemset, transaction_list):\n",
    "    \"\"\"Calculate support for an itemset in given list of transactions.\"\"\"\n",
    "    return sum(1 for transaction in transaction_list if itemset.issubset(transaction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk43pab8DEbV"
   },
   "source": [
    "## Results Display\n",
    "\n",
    "For each algorithm, we print the rules found, sorted by the lift metric to identify the most relevant associations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_wE3AoPYC27Y",
    "outputId": "be9684cd-8d34-4e0e-eacf-5697a8863eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available stores:\n",
      "1. Bookstore\n",
      "2. Clothing Store\n",
      "3. Electronics Store\n",
      "4. Grocery Store\n",
      "5. Sports Equipment Store\n",
      "Please select your store (1-5): 1\n",
      "Enter the minimum support (as a decimal, e.g., 0.01 for 1%): 0.3\n",
      "Enter the minimum confidence (as a decimal, e.g., 0.7 for 70%): 0.8\n",
      "Brute Force - Duration: 3.81s, Itemsets Found: 32\n",
      "Concating the rules to only 20..\n",
      "Rule 38: frozenset({' Fiction Novel'}) -> frozenset({' Biography', ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.8571\n",
      "Rule 37: frozenset({' Biography', ' Art Book'}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.8571\n",
      "Rule 39: frozenset({' Biography'}) -> frozenset({' Fiction Novel', ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 11: frozenset({\" Children's Book\"}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 31: frozenset({' Fiction Novel', ' Art Book'}) -> frozenset({\" Children's Book\"}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 34: frozenset({\" Children's Book\"}) -> frozenset({' Fiction Novel', ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 36: frozenset({' Fiction Novel', ' Art Book'}) -> frozenset({' Biography'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 33: frozenset({' Fiction Novel'}) -> frozenset({\" Children's Book\", ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 15: frozenset({' Biography'}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 14: frozenset({' Fiction Novel'}) -> frozenset({' Biography'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 32: frozenset({\" Children's Book\", ' Art Book'}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 10: frozenset({' Fiction Novel'}) -> frozenset({\" Children's Book\"}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 12: frozenset({' Fiction Novel'}) -> frozenset({' Art Book'}), Support: 0.3500, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 19: frozenset({\" Children's Book\", ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 30: frozenset({' Fiction Novel', \" Children's Book\"}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 35: frozenset({' Fiction Novel', ' Biography'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 25: frozenset({' Fiction Novel', ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 22: frozenset({' History Book', ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3500, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 16: frozenset({' Poetry Collection', ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 9: frozenset({\" Children's Book\"}) -> frozenset({' Art Book'}), Support: 0.3500, Confidence: 1.0000, Lift: 2.2222\n",
      "Apriori - Duration: 0.00s, Rules Found: 39\n",
      "Concating the rules to only 20..\n",
      "Rule 20: frozenset({' Fiction Novel'}) -> frozenset({' Biography', ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.8571\n",
      "Rule 17: frozenset({' Biography', ' Art Book'}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.8571\n",
      "Rule 10: frozenset({' Fiction Novel'}) -> frozenset({\" Children's Book\"}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 26: frozenset({\" Children's Book\", ' Art Book'}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 25: frozenset({' Fiction Novel', ' Art Book'}) -> frozenset({\" Children's Book\"}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 19: frozenset({' Biography'}) -> frozenset({' Fiction Novel', ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 18: frozenset({' Fiction Novel', ' Art Book'}) -> frozenset({' Biography'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 28: frozenset({\" Children's Book\"}) -> frozenset({' Fiction Novel', ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 11: frozenset({\" Children's Book\"}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 27: frozenset({' Fiction Novel'}) -> frozenset({\" Children's Book\", ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 8: frozenset({' Fiction Novel'}) -> frozenset({' Biography'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 7: frozenset({' Biography'}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 35: frozenset({' Poetry Collection', ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 2: frozenset({\" Children's Book\"}) -> frozenset({' Art Book'}), Support: 0.3500, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 24: frozenset({' Fiction Novel', \" Children's Book\"}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 29: frozenset({' Fiction Novel', ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 21: frozenset({\" Children's Book\", ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 5: frozenset({' Fiction Novel'}) -> frozenset({' Art Book'}), Support: 0.3500, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 16: frozenset({' Biography', ' Fiction Novel'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 32: frozenset({' History Book', ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3500, Confidence: 1.0000, Lift: 2.2222\n",
      "FP-Growth - Duration: 0.00s, Rules Found: 39\n",
      "Concating the rules to only 20..\n",
      "Rule 27: frozenset({' Fiction Novel'}) -> frozenset({' Biography', ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.8571\n",
      "Rule 24: frozenset({' Biography', ' Art Book'}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.8571\n",
      "Rule 20: frozenset({' Biography'}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 30: frozenset({\" Children's Book\"}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 25: frozenset({' Fiction Novel', ' Art Book'}) -> frozenset({' Biography'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 26: frozenset({' Biography'}) -> frozenset({' Fiction Novel', ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 21: frozenset({' Fiction Novel'}) -> frozenset({' Biography'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 29: frozenset({' Fiction Novel'}) -> frozenset({\" Children's Book\"}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 33: frozenset({' Fiction Novel', ' Art Book'}) -> frozenset({\" Children's Book\"}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 34: frozenset({\" Children's Book\", ' Art Book'}) -> frozenset({' Fiction Novel'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 35: frozenset({' Fiction Novel'}) -> frozenset({\" Children's Book\", ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 36: frozenset({\" Children's Book\"}) -> frozenset({' Fiction Novel', ' Art Book'}), Support: 0.3000, Confidence: 0.8571, Lift: 2.4490\n",
      "Rule 32: frozenset({' Fiction Novel', \" Children's Book\"}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 12: frozenset({' Poetry Collection', ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 28: frozenset({\" Children's Book\"}) -> frozenset({' Art Book'}), Support: 0.3500, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 15: frozenset({' Fiction Novel'}) -> frozenset({' Art Book'}), Support: 0.3500, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 17: frozenset({' Fiction Novel', ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 37: frozenset({\" Children's Book\", ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 3: frozenset({' History Book', ' Cookbook'}) -> frozenset({' Art Book'}), Support: 0.3500, Confidence: 1.0000, Lift: 2.2222\n",
      "Rule 23: frozenset({' Biography', ' Fiction Novel'}) -> frozenset({' Art Book'}), Support: 0.3000, Confidence: 1.0000, Lift: 2.2222\n",
      "Timing Performance:\n",
      "Brute Force - Duration: 3.81s\n",
      "Apriori - Duration: 0.00s\n",
      "FP-Growth - Duration: 0.00s\n"
     ]
    }
   ],
   "source": [
    "def print_rules(rules):\n",
    "    if not rules.empty:\n",
    "        if(len(rules) > 20):\n",
    "            print(\"Concating the rules to only 20..\")\n",
    "        rules_sorted = rules.sort_values(by='lift', ascending=False).head(20)\n",
    "        for index, rule in rules_sorted.iterrows():\n",
    "            print(f\"Rule {index + 1}: {rule['antecedents']} -> {rule['consequents']}, \"\n",
    "                  f\"Support: {rule['support']:.4f}, Confidence: {rule['confidence']:.4f}, Lift: {rule['lift']:.4f}\")\n",
    "    else:\n",
    "        print(\"No rules generated.\")\n",
    "\n",
    "def main():\n",
    "    store_options = ['Bookstore', 'Clothing Store', 'Electronics Store', 'Grocery Store', 'Sports Equipment Store']\n",
    "    print(\"Available stores:\")\n",
    "    for i, option in enumerate(store_options, 1):\n",
    "        print(f\"{i}. {option}\")\n",
    "\n",
    "    store_index = int(input(\"Please select your store (1-5): \")) - 1\n",
    "    transactions_file = f\"{store_options[store_index]}_transactions.csv\"\n",
    "    transactions = load_transactions(transactions_file)\n",
    "\n",
    "    min_support = float(input(\"Enter the minimum support (as a decimal, e.g., 0.01 for 1%): \"))\n",
    "    min_confidence = float(input(\"Enter the minimum confidence (as a decimal, e.g., 0.7 for 70%): \"))\n",
    "\n",
    "    # Running all algorithms\n",
    "    # run_all_algorithms(transactions, min_support, min_confidence)\n",
    "    start_time = time.time()\n",
    "    bf_itemsets = brute_force_frequent_itemsets(transactions, min_support)\n",
    "    bf_duration = time.time() - start_time\n",
    "    print(f\"Brute Force - Duration: {bf_duration:.2f}s, Itemsets Found: {len(bf_itemsets)}\")\n",
    "    if not bf_itemsets.empty:\n",
    "        bf_rules = generate_rules(bf_itemsets, min_confidence)\n",
    "        print_rules(bf_rules)\n",
    "    else:\n",
    "        print(\"No frequent itemsets found with brute force.\")\n",
    "\n",
    "    # Apriori\n",
    "    start_time = time.time()\n",
    "    ap_itemsets = run_mlxtend_algorithm(transactions, min_support, 'apriori')\n",
    "    ap_duration = time.time() - start_time\n",
    "    ap_rules = generate_rules(ap_itemsets, min_confidence)\n",
    "    print(f\"Apriori - Duration: {ap_duration:.2f}s, Rules Found: {len(ap_rules)}\")\n",
    "    print_rules(ap_rules)\n",
    "\n",
    "    # FP-Growth\n",
    "    start_time = time.time()\n",
    "    fp_itemsets = run_mlxtend_algorithm(transactions, min_support, 'fpgrowth')\n",
    "    fp_duration = time.time() - start_time\n",
    "    fp_rules = generate_rules(fp_itemsets, min_confidence)\n",
    "    print(f\"FP-Growth - Duration: {fp_duration:.2f}s, Rules Found: {len(fp_rules)}\")\n",
    "    print_rules(fp_rules)\n",
    "\n",
    "    print(\"Timing Performance:\")\n",
    "    print(f\"Brute Force - Duration: {bf_duration:.2f}s\")\n",
    "    print(f\"Apriori - Duration: {ap_duration:.2f}s\")\n",
    "    print(f\"FP-Growth - Duration: {fp_duration:.2f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3525
    },
    "id": "eIG8EP2KDtzJ",
    "outputId": "9afbfc16-4027-4b72-fba6-a7d4391195d7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFYuP7drFprO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
